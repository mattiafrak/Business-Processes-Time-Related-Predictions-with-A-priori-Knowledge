"""
this script takes as input the LSTM or RNN weights found by train.py
change the path in the shared_variables.py to point to the h5 file
with LSTM or RNN weights generated by train.py

The script is expanded to also use the elapsed time attribute
"""

from __future__ import division

import csv
import time
from collections import Counter
from datetime import datetime, timedelta
from itertools import izip
import os

import distance
import numpy as np
from math import sqrt
from keras.models import load_model
from sklearn import metrics
import shared_variables
from support_scripts.prepare_data import repetitions, amplify
from support_scripts.prepare_data_time import select_declare_verified_traces, prepare_testing_data


def run_experiments(log_name, models_folder, fold):
    model_filename = shared_variables.extract_last_model_checkpoint(log_name, models_folder, fold, 'CFRT')
    declare_model_filename = shared_variables.extract_declare_model_filename(log_name)

    log_settings_dictionary = shared_variables.log_settings[log_name]
    prefix_size_pred_from = log_settings_dictionary['prefix_size_pred_from']
    prefix_size_pred_to = log_settings_dictionary['prefix_size_pred_to']

    start_time = time.time()

    # prepare the data N.B. maxlen == predict_size
    lines, \
        lines_id, \
        lines_group, \
        lines_time, \
        lines_t, \
        lines_t2, \
        lines_t3, \
        lines_t4, \
        maxlen, \
        chars, \
        chars_group,\
        chars_time, \
        char_indices, \
        char_indices_group, \
        char_indices_time, \
        divisor, \
        divisor2, \
        predict_size, \
        target_indices_char, \
        target_indices_char_group,\
        target_indices_char_time, \
        target_char_indices, \
        target_char_indices_group, \
        target_char_indices_time = prepare_testing_data(log_name)

    # load model, set this to the model generated by train.py
    model = load_model(model_filename)


    # define helper functions
    # this one encodes the current sentence into the onehot encoding
    # noinspection PyUnusedLocal
    def encode(sentence, sentence_group, sentence_time, times_enc, times3_enc, maxlen_enc=maxlen):
        num_features = len(chars)+len(chars_group)+len(chars_time)+5
        x = np.zeros((1, maxlen_enc, num_features), dtype=np.float32)
        leftpad = maxlen_enc-len(sentence)
        times2_enc = np.cumsum(times_enc)
        for v, char in enumerate(sentence):
            midnight = times3_enc[v].replace(hour=0, minute=0, second=0, microsecond=0)
            timesincemidnight = times3_enc[v]-midnight
            multiset_abstraction = Counter(sentence[:v+1])
            for c in chars:
                if c == char:
                    x[0, v+leftpad, char_indices[c]] = 1
            for g in chars_group:
                if g == sentence_group[v]:
                    x[0, v+leftpad, len(char_indices)+char_indices_group[g]] = 1
            for y in chars_time:
                if y == sentence_time[v]:
                    x[0, v+leftpad, len(char_indices)+len(char_indices_group)+char_indices_time[y]] = 1
            x[0, v+leftpad, len(chars)+len(chars_group)+len(chars_time)] = v+1
            x[0, v+leftpad, len(chars)+len(chars_group)+len(chars_time)+1] = times_enc[v]/divisor
            x[0, v+leftpad, len(chars)+len(chars_group)+len(chars_time)+2] = times2_enc[v]/divisor2
            x[0, v+leftpad, len(chars)+len(chars_group)+len(chars_time)+3] = timesincemidnight.seconds/86400
            x[0, v+leftpad, len(chars)+len(chars_group)+len(chars_time)+4] = times3_enc[v].weekday()/7
        return x

    # modify to be able to get second best prediction
    def get_symbol_ampl(predictions, target_indices_char, target_char_indices, start_of_the_cycle_symbol,
                        stop_symbol_probability_amplifier_current, ith_best=0):
        a_pred = list(predictions)
        if start_of_the_cycle_symbol in target_char_indices:
            place_of_starting_symbol = target_char_indices[start_of_the_cycle_symbol]
            a_pred[place_of_starting_symbol] = a_pred[
                                                   place_of_starting_symbol] / stop_symbol_probability_amplifier_current
        i = np.argsort(a_pred)[len(a_pred) - ith_best - 1]
        return target_indices_char[i]

    def get_symbol_group(predictions, vth_best=0):
        v = np.argsort(predictions)[len(predictions) - vth_best - 1]
        return target_indices_char_group[v]

    def get_symbol_time(predictions, vth_best=0):
        v = np.argsort(predictions)[len(predictions) - vth_best - 1]
        return target_indices_char_time[v]

    one_ahead_gt = []
    one_ahead_pred = []
    gt_array = []

    # find cycles and modify the probability functionality goes here
    stop_symbol_probability_amplifier_current = 1
    start_of_the_cycle_symbol = " "

    folder_path = 'output_files/' + models_folder + '/' + str(fold) + '/results/baseline/'
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
    output_filename = folder_path + '%s_%s.csv' % (log_name, 'CFRT_CYCLE')

    with open(output_filename, 'wb') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        spamwriter.writerow(["Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                             "Ground truth times", "Prefix", "Predicted times", "RMSE", "MAE", "Median AE", "Ground Truth Group",
                             "Predicted Group", "Damerau-Levenshtein Resource", "Ground Truth Elapsed Time",
                             "Predicted Elapsed Time", "Damerau-Levenshtein Elapsed Time"])
        for prefix_size in range(prefix_size_pred_from, prefix_size_pred_to):
            print("prefix size: " + str(prefix_size))

            lines_s,\
            lines_id_s,\
            lines_group_s, \
            lines_time_s, \
            lines_t_s, \
            lines_t2_s, \
            lines_t3_s,\
            lines_t4_s = select_declare_verified_traces(declare_model_filename,
                                                        lines,
                                                        lines_id,
                                                        lines_group,
                                                        lines_time,
                                                        lines_t,
                                                        lines_t2,
                                                        lines_t3,
                                                        lines_t4,
                                                        prefix_size)

            print("formulas verified: " + str(len(lines_s)) + " out of : " + str(len(lines)))
            pt_array = []
            for line, line_id, line_group, line_time, times, times2, times3, times4 in izip(lines_s,
                                                                                             lines_id_s,
                                                                                             lines_group_s,
                                                                                             lines_time_s,
                                                                                             lines_t_s,
                                                                                             lines_t2_s,
                                                                                             lines_t3_s,
                                                                                             lines_t4_s):
                times.append(0)
                cropped_line = ''.join(line[:prefix_size])
                cropped_line_group = ''.join(line_group[:prefix_size])
                cropped_line_time = ''.join(line_time[:prefix_size])
                #print(times)
                cropped_times = times[:prefix_size]
                #print(cropped_times)
                cropped_times3 = times3[:prefix_size]
                cropped_times4 = times4[:prefix_size]
                if len(times2) < prefix_size:
                    continue  # make no prediction for this case, since this case has ended already
                ground_truth = ''.join(line[prefix_size:prefix_size+predict_size])
                ground_truth_group = ''.join(line_group[prefix_size:prefix_size+predict_size])
                ground_truth_time = ''.join(line_time[prefix_size:prefix_size+predict_size])
                ground_truth_t = times2[prefix_size-1]
                case_end_time = times2[len(times2)-1]
                ground_truth_t = case_end_time-ground_truth_t
                gt_array.append(ground_truth_t)
                ground_truth_t_string = ''.join(str(times[:(predict_size-1)]))
                ground_truth_t_string = ground_truth_t_string.replace(",", " ")
                prefix_string = ''.join(str(times[:prefix_size]))
                prefix_string = prefix_string.replace(',', ' ')
                predicted = ''
                predicted_group = ''
                predicted_elapsed_time = ''
                total_predicted_time = 0
                predicted_time_string = ''
                for i in range(predict_size):
                    enc = encode(cropped_line, cropped_line_group, cropped_line_time, cropped_times, cropped_times3)
                    y = model.predict(enc, verbose=0)  # make predictions
                    # split predictions into separate activity and time predictions
                    y_char = y[0][0]
                    y_group = y[1][0]
                    y_time = y[2][0]
                    y_t = y[3][0][0]
                    #print('y',y)
                    # print('activity', y_char)
                    # print('resource', y_group)
                    # print('elapsed time', y[2][0])
                    # # print('time', y_t)
                    # print('y1', y[1][0][0])
                    # print('y2', y[2][0][0])
                    # print('y3', y[3][0])
                    # print('y3', y[3][1][0])
                    # print('y3', y[3][2][0])
                    # print('y3', y[3][3][0])
                    prediction = get_symbol_ampl(y_char, target_indices_char, target_char_indices,
                                               start_of_the_cycle_symbol,
                                               stop_symbol_probability_amplifier_current)  # undo one-hot encoding
                    prediction_group = get_symbol_group(y_group)  # undo one-hot encoding
                    prediction_elapsed_time = get_symbol_time(y_time) # undo one-hot encoding
                    cropped_line += prediction
                    cropped_line_group += prediction_group
                    cropped_line_time += prediction_elapsed_time

                    stop_symbol_probability_amplifier_current, start_of_the_cycle_symbol = amplify(cropped_line)

                    # adds a fake timestamp to the list
                    t = time.strptime(cropped_times4[-1], "%Y-%m-%d %H:%M:%S")
                    new_timestamp = datetime.fromtimestamp(time.mktime(t)) + timedelta(0, 2000)
                    cropped_times4.append(new_timestamp.strftime("%Y-%m-%d %H:%M:%S"))

                    if y_t < 0:
                        y_t = 0
                    cropped_times.append(y_t)
                    # end of case was just predicted, therefore, stop predicting further into the future
                    if prediction == '!':
                        one_ahead_pred.append(total_predicted_time)
                        one_ahead_gt.append(ground_truth_t)
                        # print('! predicted, end case')
                        break
                    y_t = y_t * divisor
                    cropped_times3.append(cropped_times3[-1] + timedelta(seconds=y_t))
                    #cropped_times3.append(cropped_times3[-1] + y_t)
                    total_predicted_time = total_predicted_time + y_t
                    predicted += prediction
                    predicted_group += prediction_group
                    predicted_elapsed_time += prediction_elapsed_time
                    predicted_time_string += str(y_t)+" "
                    pt_array.append(y_t)

                lp = len(pt_array) #length of predicted times array
                lt = len(times[prefix_size:prefix_size+lp]) #length of ground truth array, reduced to predicted times one
                output = []

                if len(ground_truth) > 0:
                    output.append(prefix_size)
                    output.append(unicode(ground_truth).encode("utf-8"))
                    output.append(unicode(predicted).encode("utf-8"))
                    output.append(1 - distance.nlevenshtein(predicted, ground_truth))
                    output.append(1 - distance.jaccard(predicted, ground_truth))
                    output.append(ground_truth_t_string)
                    output.append(prefix_string)
                    output.append(predicted_time_string)
                    output.append(sqrt(metrics.mean_squared_error([times[prefix_size:prefix_size+lp]], [pt_array[:lt]])))
                    output.append(metrics.mean_absolute_error([ground_truth_t], [total_predicted_time]))
                    output.append(metrics.median_absolute_error([ground_truth_t], [total_predicted_time]))
                    output.append(unicode(ground_truth_group).encode("utf-8"))
                    output.append(unicode(predicted_group).encode("utf-8"))
                    output.append(1 - distance.nlevenshtein(predicted_group, ground_truth_group))
                    output.append(unicode(ground_truth_time).encode("utf-8"))
                    output.append(unicode(predicted_elapsed_time).encode("utf-8"))
                    output.append(1 - distance.nlevenshtein(predicted_elapsed_time, ground_truth_time))
                    spamwriter.writerow(output)
    print("TIME TO FINISH --- %s seconds ---" % (time.time() - start_time))
